{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-eaf2c185-bd61-4747-a8a9-a8499790a2ed",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": null
    },
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     2
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 450936,
    "execution_start": 1646752631669,
    "source_hash": "bd190a5a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100%; !important } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100%; !important } </style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-c358cce6-2aff-4654-b7eb-eb6f0810073f",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 6
    },
    "deepnote_cell_height": 82,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Negamax with Alpha-Beta Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "471b7a1b3bda471da0106b10724becf0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 316.96875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Overview\n",
    "**Alpha-beta pruning** is an optimization that can be applied to the negamax search algorithm. It was described and implemented in various ways throughout history. According to [Knuth, D. E. and Moore, R. W. (1975)](Bibliography.ipynb#KM75), it was John McCarthy who first thought of the concept of pruning a search tree in the context of a chess algorithm, when he criticized Alex Bernstein for his more naive chess program during the Dartmouth Summer Research Conference on Artificial Intelligence in 1956. After that, the first published discussion of alpha-beta pruning was made by Allen Newell, Herbert Simon and Cliff Shaw. Donald E. Knuth and Ronald W. Moore present a more refined version of alpha-beta pruning in their paper. The algorithm presented in this notebook is based on this refined version. \n",
    "\n",
    "Alpha-beta pruning aims to \"cut off\" (prune) any branches in the search tree that cannot affect the outcome, therefore not evaluating these branches and speeding up the search. The algorithm achieves this by keeping track of two additional parameters during the search, which are denoted as $\\alpha$ and $\\beta$. $\\alpha$ can be considered the lower bound; it is the lowest possible value that the search could output at that time. In contrast, $\\beta$ can be considered the upper bound; it is the highest possible value that the search could output at that point in time. $\\alpha$ and $\\beta$ are regularly updated during the search to narrow down the possible range of values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f83a41d59ba74b9299b2cde13fa5937c",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 381.25,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Finding the Best Value\n",
    "Based on this idea, we can define a new function $\\texttt{bestValueAlphaBeta}$ with the following signature:\n",
    "\n",
    "$$\\texttt{bestValueAlphaBeta: States}\\times \\texttt{Players} \\times \\{-1, 1\\} \\times \\mathbb{N} \\cup \\{0\\} \\times  \\texttt{Paths} \\times \\mathbb{Z} \\times \\mathbb{Z} \\rightarrow \\mathbb{Z}$$\n",
    "\n",
    "$\\texttt{bestValueAlphaBeta}$ approximates the function $\\texttt{bestValue}$ as described in [NegamaxAlgorithm.ipynb](NegamaxAlgorithm.ipynb#Negamax-Search), but applies the aforementioned alpha-beta pruning optimization. The parameters of $\\texttt{bestValueAlphaBeta}$ are the same as those of $\\texttt{bestValue}$, but with the addition of $\\alpha$ and $\\beta$. We can define $\\texttt{bestValueAlphaBeta}$ based on three cases:\n",
    "\n",
    "1. $\\alpha < \\texttt{bestValue}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}) < \\beta \\ \\rightarrow\\ \\texttt{bestValueAlphaBeta}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}, \\alpha, \\beta) = \\texttt{bestValue}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path})$\n",
    "1. $\\texttt{bestValue}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}) \\leq \\alpha \\rightarrow \\texttt{bestValueAlphaBeta}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}, \\alpha, \\beta) \\leq \\alpha$\n",
    "1. $\\beta \\leq \\texttt{bestValue}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}) \\rightarrow \\beta \\leq \\texttt{bestValueAlphaBeta}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}, \\alpha, \\beta)$\n",
    "\n",
    "As a side note, if the upper bound $\\texttt{ub}$ and the lower bound $\\texttt{lb}$ of $\\texttt{bestValue}$ are known, the following is true:\n",
    "\n",
    "$$\\texttt{bestValue}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}) := \\texttt{bestValueAlphaBeta}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}, \\texttt{lb}, \\texttt{ub})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5117b4ee2da43bcaa8a75014290acce",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 458.21875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Our Implementation\n",
    "Naturally, there are multiple implementations that meet this definition. This notebook presents one such implementation, which utilizes these properties in an attempt to improve performance compared to negamax search without alpha-beta pruning.\n",
    "\n",
    "Our implementation is given by the function `AlphaBetaPruning.get_best_move`, which has the same parameters as `NegamaxSearch.get_best_move` with the addition of $\\alpha$ and $\\beta$ values. Like in regular negamax search, `ai_turn = True` depicts that the current player tries to maximize the score and `ai_turn = False` depicts that the current player tries to minimize the score. It handles alpha-beta pruning as follows:\n",
    "1. If `ai_turn = True` and `score >= beta`, end the search and return the current result. Per the definition above, if the score is greater than or equal to $\\beta$, we can cut the search short, saving computation time.\n",
    "1. If `ai_turn = True` and `score > alpha`, set $\\alpha$ to the current score. This is the minimum score that the maximizing player can achieve.\n",
    "1. If `ai_turn = False` and `score <= alpha`, end the search and return the current result. Per the definition above, if the score is less than or equal to $\\alpha$, we can cut the search short, saving computation time.\n",
    "1. If `ai_turn = False` and `score < beta`, set $\\beta$ to the current score. This is the maximum score that the minimizing player can achieve.\n",
    "\n",
    "Optionally, this function implements memoization. This works slightly differently from memoization in standard negamax search and is detailed [later in this notebook](AlphaBetaAlgorithm.ipynb#Memoization-For-Alpha-Beta-Pruning).\n",
    "\n",
    "Furthermore, this function implements singular value extension, which works the same as in standard negamax search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-3f544fa2-3e1d-4cf9-85f4-553b75404d38",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 12
    },
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-bddf5d66-c8ba-4ae2-be82-34d272432057",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 24
    },
    "deepnote_cell_height": 225,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     2,
     null,
     2,
     null,
     2,
     null,
     2
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 769,
    "execution_start": 1652782882504,
    "output_cleared": true,
    "source_hash": "9a755bae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.gaviota\n",
    "from enum import Enum\n",
    "\n",
    "import import_ipynb\n",
    "import Evaluation\n",
    "import Util\n",
    "from Globals import *\n",
    "from ChessAlgorithm import ChessAlgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ee56cd6195d745ebb4cd04d70760af99",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 100.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### AlphaBetaPruning Class\n",
    "A class that implements the negamax search algorithm with alpha-beta pruning as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ef53f7a64d844ce48a6835c61114b4be",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1652782994761,
    "source_hash": "7e94e02a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AlphaBetaPruning(ChessAlgorithm):\n",
    "    enable_quiescence = False # Quiescence search is disabled in this version;\n",
    "                              # more details can be found at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-32528de8-188f-4576-b697-033fd4194f00",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 30
    },
    "deepnote_cell_height": 764.59375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruning.get_best_move\n",
    "Finds the best move to make based on the negamax search algorithm with alpha-beta pruning.\n",
    "\n",
    "__This function is implemented recursively, but indirectly in the following sequence:__\n",
    "* `AlphaBetaPruning.get_best_move` calls `AlphaBetaPruning.evaluate_current_node` to find the best move at the current node in the search tree.\n",
    "* `AlphaBetaPruning.evaluate_current_node` calls `AlphaBetaPruning.evaluate_moves` to go through all legal moves at the current node and evaluate them.\n",
    "* `AlphaBetaPruning.evaluate_moves` calls `AlphaBetaPruning.get_best_move` to find the best move to make after the move it's evaluating, i.e. the best move at a child node.\n",
    "\n",
    "###### <u>__Arguments__</u>\n",
    "``alpha (int):``  \n",
    "The \"alpha\" value. When the function is first called, this value should be strongly negative.  \n",
    "\n",
    "``beta (int):``  \n",
    "The \"beta\" value. When the function is first called, this value should be strongly positive.  \n",
    "\n",
    "``ai_turn (bool):``  \n",
    "Whether or not the current turn is of the AI that started the search. If this is the case, the score should be maximized. Otherwise, the score should be minimized.  \n",
    " \n",
    "``depth (int):``  \n",
    "The remaining depth of the search, i.e. how many half-moves the search should still look into the future.  \n",
    "\n",
    "``last_eval_score (int):``  \n",
    "The score provided by the previous evaluation in the search.  \n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The recommended best move to make.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place.\n",
    "\n",
    "###### __<u>Side effects</u>__\n",
    "- If a cache dictionary for memoization is used, new values may be added to this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-b7c8cf8f-e01f-4e59-9631-2964ed372358",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 36
    },
    "deepnote_app_is_code_hidden": false,
    "deepnote_cell_height": 693,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 19,
    "execution_start": 1652783692698,
    "is_code_hidden": false,
    "source_hash": "a7bca2da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_move(\n",
    "    self,\n",
    "    alpha: int,\n",
    "    beta: int,\n",
    "    ai_turn: bool,\n",
    "    depth: int,\n",
    "    last_eval_score: int\n",
    ") -> (int, chess.Move, bool, list):\n",
    "\n",
    "    state = self.board.get_state_repr()\n",
    "    color = self.board.turn\n",
    "    original_color = color if ai_turn else not color\n",
    "    \n",
    "    # Read cached result if available\n",
    "    if self.cache is not None:                \n",
    "        alpha, beta, cached_entry = self.get_cache(depth, state, alpha, beta)\n",
    "        if cached_entry is not None: return cached_entry\n",
    "\n",
    "    # Get result if the search has finished (if max depth has been reached,\n",
    "    # or if the game has ended)\n",
    "    result_score = self.board.get_search_result_if_finished(original_color, depth, last_eval_score)\n",
    "    if result_score is not None:\n",
    "        return result_score, None, False, []\n",
    "\n",
    "    # Get result if the search has not finished\n",
    "    result = self.evaluate_current_node(alpha, beta, ai_turn, depth, last_eval_score)\n",
    "\n",
    "    # Store result in cache\n",
    "    if self.cache is not None:\n",
    "        self.store_cache(depth, state, alpha, beta, result)\n",
    "        \n",
    "    return result\n",
    "\n",
    "AlphaBetaPruning.get_best_move = get_best_move\n",
    "del get_best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c1ab600b082f4b16b169cee7b1acbe2e",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 467.59375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruning.evaluate_current_node\n",
    "Finds the best move at the current node in the alpha-beta pruning search. This assumes that the search has not ended at this node, and that the result cannot be retrieved from the cache.\n",
    "\n",
    "__This function is implemented recursively, but indirectly in the following sequence:__\n",
    "* `AlphaBetaPruning.get_best_move` calls `AlphaBetaPruning.evaluate_current_node` to find the best move at the current node in the search tree.\n",
    "* `AlphaBetaPruning.evaluate_current_node` calls `AlphaBetaPruning.evaluate_moves` to go through all legal moves at the current node and evaluate them.\n",
    "* `AlphaBetaPruning.evaluate_moves` calls `AlphaBetaPruning.get_best_move` to find the best move to make after the move it's evaluating, i.e. the best move at a child node.\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "The arguments are the same as in ``AlphaBetaPruning.get_best_move``. To save space, these will not be repeated here.\n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The recommended best move to make.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f21f33713c0547c49712d01d002ac496",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 315,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1652783840949,
    "source_hash": "3d3e970a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_current_node(\n",
    "    self,\n",
    "    alpha: int,\n",
    "    beta: int,\n",
    "    ai_turn: bool,\n",
    "    depth: int,\n",
    "    last_eval_score: int\n",
    ") -> (int, chess.Move, bool, list):\n",
    "\n",
    "    available_moves = self.get_move_list(ai_turn, depth)\n",
    "    return self.evaluate_moves(available_moves, alpha, beta, ai_turn, depth, last_eval_score)\n",
    "\n",
    "AlphaBetaPruning.evaluate_current_node = evaluate_current_node\n",
    "del evaluate_current_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ce742319386f4ba4abfd89aae852e028",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 595.59375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruning.get_move_list\n",
    "Returns a list of legal moves at the current board state. This is its own function because progressive deepening requires this list to be sorted, which is described in further detail in a [later section](AlphaBetaAlgorithm.ipynb#Progressive-deepening). For regular alpha-beta pruning, the list is not sorted.\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "``ai_turn (bool):``  \n",
    "Whether or not it's the turn of the AI that started the search.  \n",
    "\n",
    "``depth (int):``  \n",
    "The remaining depth of the search, i.e. how many half-moves the search should still look into the future.  \n",
    "\n",
    "###### __<u>Returns _(list)_</u>__\n",
    "The list of legal moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3b9b6fe874c343b789c92f12afd21551",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 32,
    "execution_start": 1652785032371,
    "source_hash": "47e439f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_move_list(self, ai_turn: bool, depth: int) -> list:\n",
    "    return self.board.legal_moves\n",
    "\n",
    "AlphaBetaPruning.get_move_list = get_move_list\n",
    "del get_move_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5ec03ba916940749daf4f0dbedca52e",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 621.59375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruning.evaluate_moves\n",
    "Iterates through the available legal moves and searches their move trees using alpha-beta pruning. Once all moves are evaluated this way, the best move and its associated properties are returned.\n",
    "\n",
    "Furthermore, this function implements singular value extension. If there's only one legal move to be made, the computation time of the current node is neglegible. Therefore, if this is the case, this node does not count towards the search depth.\n",
    "\n",
    "This function also implements quiescence search, if it's enabled. More information on quiescence search can be found at the [bottom of this notebook](AlphaBetaAlgorithm.ipynb#Quiescence).\n",
    "\n",
    "__This function is implemented recursively, but indirectly in the following sequence:__\n",
    "* `AlphaBetaPruning.get_best_move` calls `AlphaBetaPruning.evaluate_current_node` to find the best move at the current node in the search tree.\n",
    "* `AlphaBetaPruning.evaluate_current_node` calls `AlphaBetaPruning.evaluate_moves` to go through all legal moves at the current node and evaluate them.\n",
    "* `AlphaBetaPruning.evaluate_moves` calls `AlphaBetaPruning.get_best_move` to find the best move to make after the move it's evaluating, i.e. the best move at a child node.\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "``available_moves (list<chess.Move>):``  \n",
    "A list of available legal moves.  \n",
    "\n",
    "The other arguments are the same as in `AlphaBetaPruning.get_best_move`. To save space, these will not be repeated here.\n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The recommended best move to make.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9d2e96678585455085225f62d86da82f",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 1125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 23,
    "execution_start": 1652784028818,
    "source_hash": "9de9ed96",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_moves(\n",
    "    self,\n",
    "    available_moves: list,\n",
    "    alpha: int,\n",
    "    beta: int,\n",
    "    ai_turn: bool,\n",
    "    depth: int,\n",
    "    last_eval_score: int\n",
    ")  -> (int, chess.Move, bool, list):\n",
    "\n",
    "    best_score = Globals.MAX_EVALUATION_SCORE * (-1 if ai_turn else 1)\n",
    "    best_move = None\n",
    "    best_move_used_endgame = False\n",
    "    best_move_path = []\n",
    "\n",
    "    use_singular_value_extension = (len(list(available_moves)) == 1)\n",
    "    for move in available_moves:\n",
    "        eval_score, used_endgame_anywhere = self.board.evaluate_move(self.use_heuristic,\n",
    "                not ai_turn, last_eval_score, depth, move, self.endgame_tablebase)\n",
    "\n",
    "        use_quiescence = (depth == 1 and self.board.is_capture(move)) \\\n",
    "            if self.enable_quiescence \\\n",
    "            else False\n",
    "\n",
    "        self.board.push(move)\n",
    "        \n",
    "        if self.board.is_game_over():\n",
    "            score_after_move = eval_score\n",
    "            new_move_path = []\n",
    "        else:\n",
    "            new_depth = depth if use_singular_value_extension or use_quiescence else depth - 1\n",
    "            score_after_move, _, used_endgame, new_move_path = self.get_best_move(\n",
    "                    alpha, beta, not ai_turn, new_depth, eval_score)\n",
    "            used_endgame_anywhere = used_endgame_anywhere or used_endgame\n",
    "                \n",
    "        self.board.pop()          \n",
    "                                                   \n",
    "        if (ai_turn and score_after_move > best_score) \\\n",
    "                or (not ai_turn and score_after_move < best_score):\n",
    "            best_score = score_after_move\n",
    "            best_move = move\n",
    "            best_move_used_endgame = used_endgame_anywhere\n",
    "            best_move_path = [move] + new_move_path\n",
    "\n",
    "        if ai_turn:\n",
    "            if best_score >= beta:\n",
    "                return best_score, best_move, best_move_used_endgame, best_move_path\n",
    "            if best_score > alpha:\n",
    "                alpha = best_score\n",
    "        else:\n",
    "            if best_score <= alpha:\n",
    "                return best_score, best_move, best_move_used_endgame, best_move_path\n",
    "            if best_score < beta:\n",
    "                beta = best_score\n",
    "                \n",
    "    return best_score, best_move, best_move_used_endgame, best_move_path\n",
    "\n",
    "AlphaBetaPruning.evaluate_moves = evaluate_moves\n",
    "del evaluate_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ff8498be4f1f4b47b6957cb41c1dd091",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 240.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruning.make_move\n",
    "Finds the best possible move according to the negamax search algorithm with alpha-beta pruning and pushes it onto the move stack.\n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The move that was made.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "465e7131292c4cb9a896cc824b036779",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 318,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_move(self) -> (int, chess.Move, bool, list):\n",
    "    score, move, used_endgame, move_path = self.get_best_move(\n",
    "        alpha=-Globals.MAX_EVALUATION_SCORE, \n",
    "        beta=Globals.MAX_EVALUATION_SCORE, \n",
    "        ai_turn=True,\n",
    "        depth=self.search_depth,\n",
    "        last_eval_score=0 # The starting board score doesn't matter,\n",
    "                          # it's evaluated by score difference\n",
    "    )\n",
    "    \n",
    "    self.board.push(move)\n",
    "    return score, move, used_endgame, move_path\n",
    "\n",
    "AlphaBetaPruning.make_move = make_move\n",
    "del make_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b58f32337cbd4b9aab3311e1e70456d0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 1050.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Memoization For Alpha-Beta Pruning\n",
    "Because the same state at the same remaining search depth may yield a different score if the values for $\\alpha$ and $\\beta$ are different, it is required to include $\\alpha$ and $\\beta$ in the cache when implementing memoization. While it would be correct to simply store these values in the cache and only use the cached values if $\\alpha$ and $\\beta$ are equal to their cached counterparts, this is not optimal. Because chess evaluation scores can vary so greatly, it is unlikely for $\\alpha$ and $\\beta$ to be exactly equal to their cached values, meaning that the cache can rarely be used. As described earlier, $\\alpha$ and $\\beta$ are bounds for evaluation scores. This property can be used to improve the percentage of cache hits while still producing a correct score, see [Stroetmann, K. (2022)](Bibliography.ipynb#KS22).\n",
    "\n",
    "In this memoization technique, cached values are tuples $(\\texttt{flag}, (\\texttt{v}, \\texttt{m}, \\texttt{e}))$.\n",
    "* $\\texttt{v} \\in \\mathbb{Z}$ is the cached move score.  \n",
    "* $\\texttt{m} \\in \\texttt{Moves}$ is the corresponding move.  \n",
    "* $\\texttt{e} \\in \\mathbb{B}$ denotes whether or not the endgame tablebases were used to find the move.  \n",
    "* $\\texttt{flag}$ can be `≤`, `=`, or `≥`.  \n",
    "\n",
    "The cache is represented by a dictionary $\\texttt{cache}$. The keys in this dictionary are tuples $(\\texttt{d}, \\texttt{s})$.\n",
    "* $\\texttt{d} \\in \\mathbb{Z}$ is the remaining search depth, i.e. how many half-moves the search can still look into the future.\n",
    "* $\\texttt{s} \\in \\texttt{States}$ is the board state at the current node in the search tree.\n",
    "\n",
    "This value tuples in the cache represent the following:\n",
    "* $\\texttt{cache}[(\\texttt{d}, \\texttt{s})] = ($`=`$, (\\texttt{v}, \\texttt{m}, \\texttt{e})) \\rightarrow \\texttt{bestValueAlphaBeta}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}, \\alpha, \\beta) = \\texttt{v}$.\n",
    "* $\\texttt{cache}[(\\texttt{d}, \\texttt{s})] = ($`≤`$, (\\texttt{v}, \\texttt{m}, \\texttt{e})) \\rightarrow \\texttt{bestValueAlphaBeta}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}, \\alpha, \\beta) \\leq \\texttt{v}$.\n",
    "* $\\texttt{cache}[(\\texttt{d}, \\texttt{s})] = ($`≥`$, (\\texttt{v}, \\texttt{m}, \\texttt{e})) \\rightarrow \\texttt{bestValueAlphaBeta}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}, \\alpha, \\beta) \\geq \\texttt{v}$.\n",
    "\n",
    "Similarly, we implement memoization based on these three cases:\n",
    "1. $\\texttt{cache}[(\\texttt{d}, \\texttt{s})] = ($`=`$, (\\texttt{v}, \\texttt{m}, \\texttt{e}))$:  \n",
    "    In this case, the move score is exactly equal to $\\texttt{v}$. Therefore, we do not change $\\alpha$ and $\\beta$ and simply use the cached result.\n",
    "\n",
    "1. $\\texttt{cache}[(\\texttt{d}, \\texttt{s})] = ($`≤`$, (\\texttt{v}, \\texttt{m}, \\texttt{e}))$:  \n",
    "    In this case, the real move score is less than or equal to $\\texttt{v}$. Here, we consider three different cases:\n",
    "    * $\\texttt{v} \\leq \\alpha$. According to the rules of alpha-beta pruning, if the real score is less than or equal to $\\alpha$, we must return a value less than or equal to $\\alpha$. Here, we return $\\texttt{v}$, but returning $\\alpha$ would also have been correct.\n",
    "    * $\\alpha < \\texttt{v} < \\beta$. Hence, the real score is less than $\\beta$. Therefore, we can use $\\texttt{v}$ as the new upper bound, i.e. we set $\\beta$ to $\\texttt{v}$.\n",
    "    * $\\texttt{v} \\geq \\beta$. This means that our current upper bound ($\\beta$) is smaller than the cached upper bound ($\\texttt{v}$), and therefore, the cached result is not of any help.\n",
    "\n",
    "1. $\\texttt{cache}[(\\texttt{d}, \\texttt{s})] = ($`≥`$, (\\texttt{v}, \\texttt{m}, \\texttt{e}))$:  \n",
    "    In this case, the real move score is greater than or equal to $\\texttt{v}$. Here, we consider three different cases:\n",
    "    * $\\texttt{v} \\geq \\beta$. According to the rules of alpha-beta pruning, if the real score is greater than or equal to $\\beta$, we must return a value greater than or equal to $\\beta$. Here, we return $\\texttt{v}$, but returning $\\beta$ would also have been correct.\n",
    "    * $\\alpha < \\texttt{v} < \\beta$. Hence, the real score is greater than $\\alpha$. Therefore, we can use $\\texttt{v}$ as the new lower bound, i.e. we set $\\alpha$ to $\\texttt{v}$.\n",
    "    * $\\texttt{v} \\leq \\alpha$. This means that our current lower bound ($\\alpha$) is greater than the cached lower bound ($\\texttt{v}$), and therefore, the cached result is not of any help.\n",
    "\n",
    "Of note is that our implementation does not use string literals for the $\\texttt{flag}$ parameter in the cached value tuple. Instead, it uses integer values (retrieved using an enumerator) for slightly better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "35e0e9b8120045699ebefe13293b04b9",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 100.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### AlphaBetaPruningMemo Class\n",
    "A class that implements the negamax search algorithm with alpha-beta pruning, including memoization as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "06e4e7568882475ba13c48e8346b3839",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 162.1875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 447,
    "execution_start": 1653296833191,
    "source_hash": "7a49ade3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AlphaBetaPruningMemo(AlphaBetaPruning):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4e159d1e49a74d4aa517d1f1ee3053e0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 246.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Comparisons Class\n",
    "An enumerator that contains values for the following comparisons:\n",
    "- Equal to\n",
    "- Less than or equal to\n",
    "- Greater than or equal to\n",
    "\n",
    "These values are used for alpha-beta pruning memoization and replace the single-character strings mentioned in the theory above, as integer constants are more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b2eafaee-876b-4f32-a562-403ea177a709",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 42
    },
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1646752647644,
    "source_hash": "f848c8ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Comparisons(Enum):\n",
    "    EQUAL = 0\n",
    "    LTE = 1 # Less than or equal to\n",
    "    GTE = 2 # Greater than or equal to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0ac11086985745db8f36a419c11fa943",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 472.59375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruningMemo.get_cache\n",
    "Retrieves a cached value in an alpha-beta pruning search, based on the equations listed in the theory above.\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "``depth (int):``  \n",
    "The remaining depth of the search, i.e. how many half-moves the search should still look into the future.  \n",
    "\n",
    "``state (tuple):``  \n",
    "A representation of the current chess board state.  \n",
    "\n",
    "``alpha (int):``  \n",
    "The current \"alpha\" value.  \n",
    "\n",
    "``beta (int):``  \n",
    "The current \"beta\" value.\n",
    "\n",
    "###### __<u>Returns _(int, int, (int, chess.Move, bool, list<chess.Move>))_</u>__\n",
    "- The new \"alpha\" value.\n",
    "- The new \"beta\" value.\n",
    "- The move score, move, whether or not the endgame tablebases were used and the predicted move sequence; or `None` if no value could be read from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a6e40fb8-e9fc-48a1-8828-a6bb03a80018",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 48
    },
    "deepnote_cell_height": 639,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 15,
    "execution_start": 1652784155651,
    "source_hash": "56f0a72e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cache(\n",
    "    self,\n",
    "    depth: int,\n",
    "    state: tuple,\n",
    "    alpha: int,\n",
    "    beta: int\n",
    ") -> (int, int, (int, chess.Move, bool, list)):\n",
    "\n",
    "    key = (depth, state)\n",
    "    if key not in self.cache:\n",
    "        return alpha, beta, None\n",
    "\n",
    "    flag, result = self.cache[key]\n",
    "    score, _, _, _ = result\n",
    "    if flag == Comparisons.EQUAL:\n",
    "        return alpha, beta, result\n",
    "    if flag == Comparisons.LTE:\n",
    "        if score <= alpha:\n",
    "            return alpha, beta, result\n",
    "        if score < beta: # alpha < score < beta\n",
    "            return alpha, score, None\n",
    "    if flag == Comparisons.GTE:\n",
    "        if beta <= score:\n",
    "            return alpha, beta, result\n",
    "        if alpha < score: # alpha < score < beta\n",
    "            return score, beta, None\n",
    "\n",
    "    # Invalid flag or no optimization possible\n",
    "    return alpha, beta, None\n",
    "\n",
    "AlphaBetaPruningMemo.get_cache = get_cache\n",
    "del get_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "de16a7b63ffa4b968a9c7ad3420d6304",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 404.796875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruningMemo.store_cache\n",
    "Caches a board score in an alpha-beta pruning search, based on the representation of the cached tuples as described in the theory above.\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "``depth (int):``  \n",
    "The remaining depth of the search, i.e. how many half- moves the search should still look into the future.  \n",
    "\n",
    "``state (tuple):``  \n",
    "A representation of the current chess board state.  \n",
    "\n",
    "``alpha (int):``  \n",
    "The current \"alpha\" value.  \n",
    "\n",
    "``beta (int):``  \n",
    "The current \"beta\" value.  \n",
    "\n",
    "``result (tuple<int, chess.Move, bool, list<chess.Move>>):``  \n",
    "The move score, move, whether or not the endgame tablebases were used and the predicted move sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0f087a79-7cbb-4bef-95de-5562749be1a6",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 54
    },
    "deepnote_cell_height": 441,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 29,
    "execution_start": 1652784391921,
    "source_hash": "639334d5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def store_cache(\n",
    "    self,\n",
    "    depth: int,\n",
    "    state: tuple,\n",
    "    alpha: int,\n",
    "    beta: int,\n",
    "    result: tuple\n",
    ") -> None:\n",
    "\n",
    "    key = (depth, state)\n",
    "    score, _, _, _ = result\n",
    "\n",
    "    if score <= alpha:\n",
    "        self.cache[key] = (Comparisons.LTE, result)\n",
    "    elif score < beta: # alpha < score < beta\n",
    "        self.cache[key] = (Comparisons.EQUAL, result)\n",
    "    else: # beta <= score\n",
    "        self.cache[key] = (Comparisons.GTE, result)\n",
    "\n",
    "AlphaBetaPruningMemo.store_cache = store_cache\n",
    "del store_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-23576796-2219-48b5-9b85-338d5304a3fa",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 72
    },
    "deepnote_cell_height": 240.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruningMemo.make_move\n",
    "Finds the best possible move according to the negamax search algorithm with alpha-beta pruning and memoization, and pushes it onto the move stack.\n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The move that was made.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-f54aaa33-8247-43b3-9098-2890db438a8d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 78
    },
    "deepnote_cell_height": 351,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25,
    "execution_start": 1652784492558,
    "source_hash": "6e461c5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_move(self) -> (int, chess.Move, bool, list):\n",
    "    self.cache = {}\n",
    "    score, move, used_endgame, move_path = self.get_best_move(\n",
    "        alpha=-Globals.MAX_EVALUATION_SCORE, \n",
    "        beta=Globals.MAX_EVALUATION_SCORE, \n",
    "        ai_turn=True,\n",
    "        depth=self.search_depth,\n",
    "        last_eval_score=0 # The starting board score doesn't matter,\n",
    "                          # it's evaluated by score difference\n",
    "    )\n",
    "    \n",
    "    self.board.push(move)\n",
    "    return score, move, used_endgame, move_path\n",
    "\n",
    "AlphaBetaPruningMemo.make_move = make_move\n",
    "del make_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "829f866b102e4bdab8cbbe43927ecdd6",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 389.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Progressive deepening\n",
    "\n",
    "__Progressive deepening__, also known as iterative deepening, is the idea of gradually increasing the maximum search depth up to a certain number, see [Chess Programming Wiki (2019)](Bibliography.ipynb#CPW19). In the context of games, this method can be used if information from a previous search is useful for a current search, even if that previous search used a lower search depth.\n",
    "\n",
    "According to [Chess Programming Wiki (2022a)](Bibliography.ipynb#CPW22a), this is the case in alpha-beta pruning. In alpha-beta pruning, it is advantageous to evaluate the best moves first. These moves change the values of $\\alpha$ and $\\beta$ the most significantly. Therefore, more branches of the search can be pruned, decreasing the amount of nodes to evaluate and speeding up the search. If the move scores of previous searches in a progressive deepening implementation are cached using memoization, these values can be used to estimate the effectiveness of each move, allowing the current search to sort the move list using these scores. This creates a best-first ordered list of moves, allowing the aforementioned optimization to be implemented.\n",
    "\n",
    "Here, it should be taken into account whether the current player tries to maximize or minimize the score. If they want to maximize the score, the list should be sorted in descending order. If they want to minimize the score, the list should be sorted in ascending order.\n",
    "\n",
    "Because the amount of nodes that a search needs to evaluate grows exponentially with its search depth, searching the tree with a lower search depth takes comparatively little time. Generally, the time saved from sorting moves outweighs the time spent on performing these extra searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b41d9e812473410a855008327b5dd7c5",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 100.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### AlphaBetaPruningPD Class\n",
    "A class that implements the negamax search algorithm with alpha-beta pruning, memoization and progressive deepening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a1135915a3404d5abf7bdfc71c07561b",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 99,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1652784810299,
    "source_hash": "89530f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AlphaBetaPruningPD(AlphaBetaPruningMemo):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d94d0b48087b4acd9ec45bfcd35ed572",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 309.59375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruningPD.get_move_list\n",
    "Returns a list of legal moves, sorted by move score. This lets alpha-beta pruning search through the best moves first.\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "``ai_turn (bool):``  \n",
    "Whether or not it's the turn of the AI that started the search, i.e. whether the current player is maximizing or minimizing. The list will be sorted either descendingly or ascendingly depending on this value.  \n",
    "\n",
    "``depth (int):``  \n",
    "The remaining depth of the search, i.e. how many half-moves the search should still look into the future.  \n",
    "\n",
    "###### __<u>Returns _(list)_</u>__\n",
    "The sorted list of legal moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3768c34c-fd6b-4f6e-a31f-c99755b5bd9d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 66
    },
    "deepnote_cell_height": 225,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1652785327844,
    "source_hash": "21016b12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_move_list(self, ai_turn: bool, depth: int) -> list:\n",
    "    return sorted(\n",
    "        self.board.legal_moves,\n",
    "        reverse=ai_turn,\n",
    "        key=lambda move: self.value_cache(move, depth - 2)\n",
    "    )\n",
    "\n",
    "AlphaBetaPruningPD.get_move_list = get_move_list\n",
    "del get_move_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dec4c94057eb400e83ff27f44ee39422",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 287.1875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruningPD.value_cache\n",
    "Returns a cached move score if available, or a score of 0 if not. Can be used to sort moves by score.\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "``move (chess.Move):``  \n",
    "The move whose potentially cached score should be found.  \n",
    "\n",
    "``depth (int):``  \n",
    "The remaining depth of the search, i.e. how many half-moves the search should still look into the future.  \n",
    "\n",
    "###### __<u>Returns _(int)_</u>__\n",
    "The cached move score, or 0 if unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c7bc5d80-447d-4f93-bf20-fbb0f244aee5",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 60
    },
    "deepnote_cell_height": 261,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 40,
    "execution_start": 1652785342587,
    "source_hash": "fb540a63",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def value_cache(self, move: chess.Move, depth: int) -> int:\n",
    "    self.board.push(move)\n",
    "    state = self.board.get_state_repr()\n",
    "    self.board.pop()\n",
    "    \n",
    "    _, result = self.cache.get((depth, state), ('=', (0, None, False, [])))\n",
    "    score, _, _, _ = result\n",
    "    return score\n",
    "\n",
    "AlphaBetaPruningPD.value_cache = value_cache\n",
    "del value_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "18b9e12113214f8080ad21668e8cc5e7",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 262.796875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### AlphaBetaPruningPD.make_move\n",
    "Finds the best possible move according to the negamax search algorithm with alpha-beta pruning, memoization and progressive deepening, and pushes it onto the move stack.\n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The move that was made.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3f145f6b-a43b-4ed2-b0f6-485e5aed44a4",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 84
    },
    "deepnote_cell_height": 684.1875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 812,
    "execution_start": 1654254614245,
    "source_hash": "12fb690f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_move(self) -> (int, chess.Move, bool):\n",
    "    score, move, used_endgame, move_path = 0, None, False, []\n",
    "    self.cache = {}\n",
    "\n",
    "    for limit in range(1, self.search_depth + 1):\n",
    "        score, move, used_endgame, move_path = self.get_best_move(\n",
    "            alpha=-Globals.MAX_EVALUATION_SCORE,\n",
    "            beta=Globals.MAX_EVALUATION_SCORE,\n",
    "            ai_turn=True,\n",
    "            depth=limit,\n",
    "            last_eval_score=0 # The starting board score doesn't matter,\n",
    "                              # it's evaluated by score difference\n",
    "        )\n",
    "\n",
    "        # If a checkmate in favor of the AI has been found, there's no\n",
    "        # reason to search any further.\n",
    "        if score >= Globals.EVALUATION_SCORE_CHECKMATE:\n",
    "            self.board.push(move)\n",
    "\n",
    "            # The score should be calculated based on the limit and the search depth within\n",
    "            # that limit. The lower the limit the better, and the higher the search depth\n",
    "            # the better. This score matches the corresponding score from alpha-beta\n",
    "            # pruning without progressive deepening.\n",
    "            score = Globals.EVALUATION_SCORE_CHECKMATE + self.search_depth - limit + 1\n",
    "            return score, move, used_endgame, move_path\n",
    "\n",
    "    self.board.push(move)\n",
    "    return score, move, used_endgame, move_path\n",
    "\n",
    "AlphaBetaPruningPD.make_move = make_move\n",
    "del make_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4dcbc341c226462e8407203978001a6a",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 717.1875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id= 'Quiescence'></a>\n",
    "### Quiescence Search\n",
    "\n",
    "One problem with the search algorithms thus far is that they have a fixed maximum search depth. Ignoring singular value extension, these algorithms are unable to look further than the maximum search depth given to them. This can be problematic in certain circumstances. As an example, consider the following simple board state where white can make the next move.\n",
    "\n",
    "![](Images/QuiescenceExample.png)\n",
    "\n",
    "Let us assume that the white player is a search algorithm that reached this position with a remaining search depth of 1. In other words, it is only able to look one more half-move into the future. There are several legal moves to make in this position, so singular value extension is not of relevance here.\n",
    "\n",
    "One such legal move is `d2d4`, where the white queen captures the black knight. Capturing a piece results in a high evaluation score, so the search algorithm considers this a great move for white. However, it is clear that this is very inaccurate, as the black knight is protected by the black pawn on `E5`. As a result, if black makes the move `e5d4`, it can capture the white queen. This is not a favorable trade for white, making `d2d4` a terrible move in practice.\n",
    "\n",
    "If the search algorithm were to look one more half-move into the future, it could find the move `e5e4` from black and realize that `d2d4` is a bad move. However, as it stands, it is unable to do so. As far as this algorithm is concerned, the move `d2d4` lets white capture a knight, making this an excellent move. It will therefore rate the path that leads to this position much higher than it should, resulting in the algorithm potentially choosing a seemingly nonsensical move in an attempt to reach this position.\n",
    "\n",
    "In other words, these algorithms cannot recognize protected pieces if their remaining search depth is 1. A solution to this problem is **quiescence search**. The idea behind quiescence search is that a search may only end on a \"quiet\" position, see [Chess Programming Wiki (2021a)](Bibliography.ipynb#CPW21a). In our implementation, a quiet position is defined as a position where the last move was not a capture. Traditionally, checks are also considered non-quiet. However, the function `is_check` from the used chess library is implemented very inefficiently, therefore checking for check is a slow operation. From testing, we found that this additional check could make the search up to four times slower, so we decided to omit this check and only consider captures.\n",
    "\n",
    "Implementing quiescense search is similar to singular value extension in that a node does not count towards the search depth. In this case, any non-quiet node that would normally be a leaf node is not counted, with the search continuing down the branches from this node. After that, once a quiet node is reached, the search of the current branch is considered finished.\n",
    "\n",
    "Naturally, because quiescence search has to search deeper than usual in some branches, it slows down the search. As such, if quiescence search is enabled, it is recommended to use a lower search depth to compensate. This means that the search algorithm isn't able to look as far ahead for quiet positions, even if those quiet positions may lead to checkmates. However, it is also less likely to make nonsensical moves as a result of not detecting protected pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e142196e646d41a4abe0eaaca9b7f0d3",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 100.390625,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "1e489002-1d0e-4892-8dc4-49d215805343",
    "tags": []
   },
   "source": [
    "### AlphaBetaPruningQuiescence Class\n",
    "A class that implements the negamax search algorithm with alpha-beta pruning, memoization, progressive deepening and quiescence search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-c38288a6-a7cb-486c-8a4a-c0e4ea31b961",
    "deepnote_cell_height": 162.1875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1739,
    "execution_start": 1654180235425,
    "owner_user_id": "bf601137-8aff-4afc-8e92-5767ca086e79",
    "source_hash": "5709617a"
   },
   "outputs": [],
   "source": [
    "class AlphaBetaPruningQuiescence(AlphaBetaPruningPD):\n",
    "    enable_quiescence = True"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a3aa8d62-1517-4618-8cd7-29b3aa7b5c6c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
