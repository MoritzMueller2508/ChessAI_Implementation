{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00000-eaf2c185-bd61-4747-a8a9-a8499790a2ed",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": null
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 450936,
    "execution_start": 1646752631669,
    "source_hash": "bd190a5a",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 116.75,
    "deepnote_output_heights": [
     2
    ]
   },
   "source": "from IPython.core.display import HTML, display\ndisplay(HTML('<style>.container { width:100%; !important } </style>'))",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100%; !important } </style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Alpha-Beta Pruning",
   "metadata": {
    "cell_id": "00001-c358cce6-2aff-4654-b7eb-eb6f0810073f",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 6
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 81.83332824707031
   }
  },
  {
   "cell_type": "markdown",
   "source": "Stroetmann, K. (2022) describes __alpha-beta pruning__ as an optimization of the minimax search algorithm. It aims to \"cut off\" (prune) any branches in the search tree that cannot affect the outcome, therefore not evaluating these branches and speeding up the search. The algorithm achieves this by keeping track of two additional parameters during the search, which are denoted as $α$ and $β$. $α$ can be considered the lower bound; it is the lowest possible value that the search could output at that time. In contrast, $β$ can be considered the upper bound; it is the highest possible value that the search could output at that point in time. $α$ and $β$ are regularly updated during the search to narrow down the possible range of values.  \n\nBecause alpha-beta pruning is an extension of minimax search, it uses a maximizing function and a minimizing function, called $alphaBetaMax$ and $alphaBetaMin$ respectively. $alphaBetaMax$ approximates minimax's $maxValue$ as follows:\n\n1. $\\alpha \\leq \\texttt{maxValue}(s) \\leq \\beta \\;\\rightarrow\\;\\texttt{alphaBetaMax}(s, \\alpha, \\beta) = \\texttt{maxValue}(s)$\n1. $\\texttt{maxValue}(s) < \\alpha \\;\\rightarrow\\; \\texttt{alphaBetaMax}(s, \\alpha, \\beta) \\leq \\alpha$\n1. $\\beta < \\texttt{maxValue}(s) \\;\\rightarrow\\; \\beta \\leq \\texttt{alphaBetaMax}(s, \\alpha, \\beta)$\n\nSimilarly, $alphaBetaMin$ approximates minimax's $minValue$ as follows:\n\n1. $\\alpha \\leq \\texttt{minValue}(s) \\leq \\beta \\;\\rightarrow\\;\\texttt{alphaBetaMin}(s, \\alpha, \\beta) = \\texttt{minValue}(s)$\n1. $\\texttt{minValue}(s) < \\alpha \\;\\rightarrow\\; \\texttt{alphaBetaMin}(s, \\alpha, \\beta) \\leq \\alpha$\n1. $\\beta < \\texttt{minValue}(s) \\;\\rightarrow\\; \\beta \\leq \\texttt{alphaBetaMin}(s, \\alpha, \\beta)$\n\nFinally, if the upper bound `ub` and the lower bound $lb$ of $maxValue$ and $minValue$ are known, the following is true:\n\n1. $\\texttt{maxValue}(s) := \\texttt{alphaBetaMax}(s, lb, ub)$\n1. $\\texttt{minValue}(s) := \\texttt{alphaBetaMin}(s, lb, ub)$\n\nNaturally, there are multiple implementations that meet this definition. This notebook presents one such implementation, which utilizes these properties to yield a significant performance increase compared to minimax search. Like the implementation of minimax search, this alpha-beta pruning implementation uses a limit $max_iterations$ denoting the maximum search depth, which is required to make the search realistic on current hardware.\n\n---",
   "metadata": {
    "cell_id": "471b7a1b3bda471da0106b10724becf0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 650.5499877929688
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Dependencies",
   "metadata": {
    "cell_id": "00001-3f544fa2-3e1d-4cf9-85f4-553b75404d38",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 12
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 61.83332824707031
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "dda8470d-f205-42f9-a341-d545a0126f7d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 18
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18307,
    "execution_start": 1649858249035,
    "output_cleared": false,
    "source_hash": "457e246f",
    "tags": [],
    "owner_user_id": "1e489002-1d0e-4892-8dc4-49d215805343",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 547.9166870117188
   },
   "source": "!pip install python-chess\n!pip install import_ipynb",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting python-chess\n  Downloading python_chess-1.999-py3-none-any.whl (1.4 kB)\nCollecting chess<2,>=1\n  Downloading chess-1.9.0-py3-none-any.whl (147 kB)\n\u001b[K     |████████████████████████████████| 147 kB 17.8 MB/s \n\u001b[?25hInstalling collected packages: chess, python-chess\nSuccessfully installed chess-1.9.0 python-chess-1.999\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nCollecting import_ipynb\n  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\nBuilding wheels for collected packages: import-ipynb\n  Building wheel for import-ipynb (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=5eb58cf9ce15395c0220f1c3f828fff679f5938c8e3a8cc77c6a3325305b62a8\n  Stored in directory: /root/.cache/pip/wheels/b1/5e/dc/79780689896a056199b0b9f24471e3ee184fbd816df355d5f0\nSuccessfully built import-ipynb\nInstalling collected packages: import-ipynb\nSuccessfully installed import-ipynb-0.1.3\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00001-bddf5d66-c8ba-4ae2-be82-34d272432057",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 24
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 444,
    "execution_start": 1649858267342,
    "output_cleared": false,
    "source_hash": "e57b20bf",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 402.83331298828125,
    "deepnote_output_heights": [
     null,
     2,
     null,
     2
    ]
   },
   "source": "import chess\nimport chess.gaviota\nimport random\nimport signal\nimport time\nfrom typing import Union\nfrom enum import Enum\n\nimport import_ipynb\nimport Util\nfrom Globals import *",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "importing Jupyter notebook from Util.ipynb\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100%; !important } </style>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "importing Jupyter notebook from Globals.ipynb\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100%; !important } </style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.get_best_move_alphabeta\nFinds the best move to make based on the alpha-beta pruning algorithm. This algorithm works as follows:\n- Iterate over all legal moves in the current position.\n- For each move, find the best possible score after making this move.\n    - This is done by calling the alpha-beta pruning function recursively: increasing the current iteration by 1 and switching turns.\n    - If a move yields a lower score than the previously examined move in the hierarchy when maximizing, or a higher score when minimizing, it cannot be the optimal move. Therefore, such moves are not evaluated further.\n- Find the maximum (if it's the AI's turn) or the minimum (if it's other player's/AI's turn) score of all legal moves, alongside the move that was able to reach this optimized state. This is the move that the algorithm recommends.\n\nOptionally, the algorithm uses memoization, which is a type of caching. This works by mapping a board state, plus iteration details, to the corresponding score and move and storing it in a cache dictionary. If, during a later iteration, the same board state is reached on the same iteration, these values can be read from the cache. Alpha-beta pruning additionally requires one to account for the values of alpha and beta. This is further detailed later in this notebook.\n\nThis function combines the abovementioned $alphaBetaMax$ and $alphaBetaMin$ because of their functional overlap. If $ai\\_turn = True$, then the function implements $alphaBetaMax$; otherwise, it implements $alphaBetaMin$.\n\n__This function is implemented recursively, but indirectly: it calls other functions that can in turn call this function again.__\n\n###### <u>__Arguments__</u>\n``cache (dict):``  \nA cache dictionary if memoization is desired, or None if memoization should be disabled.\n\n``use_heuristic (bool):``  \nWhether or not the heuristic for evaluating the chess board should be used. Chess problems don't need this heuristic.  \n\n``alpha (int):``  \nThe \"alpha\" value in the current iteration. On the first iteration, this value should be strongly negative.  \n\n``beta (int):``  \nThe \"beta\" value in the current iteration. On the first iteration, this value should be strongly positive.  \n\n``ai_turn (bool):``  \nWhether or not the current turn is of the AI that started the search. If this is the case, the score should be maximized. Otherwise, the score should be minimized.  \n\n``color (chess.Color):``  \nThe color of the player whose turn it currently is.  \n\n``endgame_tablebase (Union[chess.gaviota.NativeTablebase, chess.gaviota.PythonTablebase]):``  \nThe endgame tablebase attached to the game, which serves as a shortcut for ideal moves in the endgame.  \n\n``iteration (int):``  \nThe depth of the search (amount of moves currently looking ahead).  \n\n``max_iterations (int): ``  \nThe maximum depth of the search.  \n\n``using_progressive_deepening (bool):``   \nWhether or not the search is part of a progressive deepening search.  \n\n``last_eval_score (int):``  \nThe score provided by the previous evaluation in the search.  \n\n###### __<u>Returns _(int, chess.Move, bool)_</u>__\n- The board score after making the recommended best move.\n- The recommended best move to make.\n- Whether or not the endgame library was used to find the move.\n\n###### __<u>Side effects</u>__\n- If a cache dictionary is provided, new values may be added to this dictionary.\n- If the search is interrupted, the board may be in a different state than when the search started.",
   "metadata": {
    "cell_id": "00003-32528de8-188f-4576-b697-033fd4194f00",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 30
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1386.2333984375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00000-b7c8cf8f-e01f-4e59-9631-2964ed372358",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 36
    },
    "deepnote_app_is_code_hidden": false,
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1649858272477,
    "is_code_hidden": false,
    "source_hash": "c8b2140a",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 890.8333129882812
   },
   "source": "def get_best_move_alphabeta(\n    self,\n    cache: dict,\n    use_heuristic: bool,\n    alpha: int,\n    beta: int,\n    ai_turn: bool,\n    color: chess.Color,\n    endgame_tablebase: Union[chess.gaviota.NativeTablebase,\n            chess.gaviota.PythonTablebase],\n    iteration: int,\n    max_iterations: int,\n    using_progressive_deepening: bool,\n    last_eval_score: int\n) -> (int, chess.Move, bool):\n\n    state = self.get_state_repr()\n    original_color = color if ai_turn else not color\n    \n    # Read cached result if available\n    if cache is not None:                \n        alpha, beta, cached_entry = self.get_alphabeta_cache(\n                cache, iteration, max_iterations, state, alpha, beta)\n        if cached_entry is not None: return cached_entry\n\n    # Get result if the search has finished (if max iterations has been reached,\n    # or if the game has ended)\n    result_score = self.get_search_result_if_finished(original_color, iteration,\n            max_iterations, last_eval_score)\n    if result_score is not None:\n        return result_score, None, False\n\n    # Get result if the search has not finished\n    result = self.value_alphabeta(cache, use_heuristic, alpha, beta, ai_turn,\n            color, endgame_tablebase, iteration, max_iterations,\n            using_progressive_deepening, last_eval_score)\n\n    # Store result in cache\n    if cache is not None:\n        self.store_alphabeta_cache(cache, iteration, max_iterations,\n                state, alpha, beta, result)\n        \n    return result\n\nchess.Board.get_best_move_alphabeta = get_best_move_alphabeta\ndel get_best_move_alphabeta",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.value_alphabeta\nFinds the best move at the current node in the alpha-beta pruning search. This assumes that the search has not ended at this node, and that the result at the current board state and iteration is not available in the cache.\n\n__This function is implemented recursively, but indirectly: it calls other functions that can in turn call this function again.__\n\n###### __<u>Arguments</u>__\nThe arguments are the same as in ``chess.Board.get_best_move_alphabeta``. To save space, these will not be repeated here.\n\n###### __<u>Returns _(int, chess.Move, bool)_</u>__\n- The board score after making the recommended best move.\n- The recommended best move to make.\n- Whether or not the endgame library was used to find the move.\n\n###### __<u>Side effects</u>__\n- If the search is interrupted, the board may be in a different state than when the search started.",
   "metadata": {
    "cell_id": "c1ab600b082f4b16b169cee7b1acbe2e",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 394.23333740234375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f21f33713c0547c49712d01d002ac496",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1649858274389,
    "source_hash": "59f263d2",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 656.8333129882812
   },
   "source": "def value_alphabeta(\n    self,\n    cache: dict,\n    use_heuristic: bool,\n    alpha: int,\n    beta: int,\n    ai_turn: bool,\n    color: chess.Color,\n    endgame_tablebase: Union[chess.gaviota.NativeTablebase,\n            chess.gaviota.PythonTablebase],\n    iteration: int,\n    max_iterations: int,\n    using_progressive_deepening: bool,\n    last_eval_score: int\n) -> (int, chess.Move, bool):\n\n    sve_result = self.single_value_extension_alphabeta(cache, use_heuristic,\n            alpha, beta, ai_turn, color, endgame_tablebase, iteration,\n            max_iterations, using_progressive_deepening, last_eval_score)\n    if sve_result is not None:\n        return sve_result\n\n    available_moves = self.get_sorted_moves(cache, ai_turn,\n                    iteration, max_iterations) \\\n            if using_progressive_deepening \\\n            else self.legal_moves\n\n    return self.evaluate_moves_alphabeta(available_moves, cache, use_heuristic,\n            alpha, beta, ai_turn, color, endgame_tablebase, iteration,\n            max_iterations, using_progressive_deepening, last_eval_score)\n\nchess.Board.value_alphabeta = value_alphabeta\ndel value_alphabeta",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.single_value_extension_alphabeta\nImplements single value extension in the alpha-beta pruning search. This is an optimization for when only one legal move can be made. In this case, this move is evaluated without increasing the iteration counter, since searching through only one node adds a neglegible amount of time.\n\n__This function is implemented recursively, but indirectly: it calls other functions that can in turn call this function again.__\n\n###### __<u>Arguments</u>__\nThe arguments are the same as in `chess.Board.get_best_move_alphabeta`. To save space, these will not be repeated here.\n\n###### __<u>Returns _(int, chess.Move, bool)_</u>__\n- The board score after making the recommended best move.\n- The recommended best move to make.\n- Whether or not the endgame library was used to find the move.\n\n###### __<u>Side effects</u>__\n- If the search is interrupted, the board may be in a different state than when the search started.",
   "metadata": {
    "cell_id": "b844672e905f4c6a9dd3b15c694f8251",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 416.6333312988281
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "15f1a7d8224340309efea01448a6f637",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1649858275565,
    "source_hash": "5b0ac2a4",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 602.8333129882812
   },
   "source": "def single_value_extension_alphabeta(\n    self,\n    cache: dict,\n    use_heuristic: bool,\n    alpha: int,\n    beta: int,\n    ai_turn: bool,\n    color: chess.Color,\n    endgame_tablebase: Union[chess.gaviota.NativeTablebase,\n            chess.gaviota.PythonTablebase],\n    iteration: int,\n    max_iterations: int,\n    using_progressive_deepening: bool,\n    last_eval_score: int\n) -> (int, chess.Move, bool):\n\n    if len(list(self.legal_moves)) != 1:\n        return None\n\n    move = list(self.legal_moves)[0]\n    self.push(move)\n    score, _, used_endgame = self.get_best_move_alphabeta(\n            cache, use_heuristic, alpha, beta, not ai_turn, not color,\n            endgame_tablebase, iteration, max_iterations,\n            using_progressive_deepening, last_eval_score)\n    self.pop()\n    return score, move, used_endgame\n\nchess.Board.single_value_extension_alphabeta = single_value_extension_alphabeta\ndel single_value_extension_alphabeta",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.evaluate_moves_alphabeta\nIterates through the available legal moves and searches their move trees using alpha-beta pruning. Once all moves are evaluated this way, the best move and its associated properties are returned.\n\n__This function is implemented recursively, but indirectly: it calls other functions that can in turn call this function again.__\n\n###### __<u>Arguments</u>__\n``available_moves (list<chess.Move>):``  \nA list of available legal moves.  \n\nThe other arguments are the same as in `chess.Board.get_best_move_alphabeta`. To save space, these will not be repeated here.\n\n###### __<u>Returns _(int, chess.Move, bool)_</u>__\n- The board score after making the recommended best move.\n- The recommended best move to make.\n- Whether or not the endgame library was used to find the move.\n\n###### __<u>Side effects</u>__\n- If the search is interrupted, the board may be in a different state than when the search started.",
   "metadata": {
    "cell_id": "f5ec03ba916940749daf4f0dbedca52e",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 475.433349609375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9d2e96678585455085225f62d86da82f",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1649858277131,
    "source_hash": "d97c0d77",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1304.8333740234375
   },
   "source": "def evaluate_moves_alphabeta(\n    self,\n    available_moves: list,\n    cache: dict,\n    use_heuristic: bool,\n    alpha: int,\n    beta: int,\n    ai_turn: bool,\n    color: chess.Color,\n    endgame_tablebase: Union[chess.gaviota.NativeTablebase,\n            chess.gaviota.PythonTablebase],\n    iteration: int,\n    max_iterations: int,\n    using_progressive_deepening: bool,\n    last_eval_score: int\n)  -> (int, chess.Move, bool):\n\n    best_score = Globals.MAX_EVALUATION_SCORE * (-1 if ai_turn else 1)\n    best_move = None\n    best_move_used_endgame = False\n\n    for move in available_moves:\n        eval_score, used_endgame_anywhere = self.evaluate_move(\n                use_heuristic, color, not ai_turn, last_eval_score,\n                iteration, move, endgame_tablebase)\n\n        is_capture = self.is_capture(move)\n        self.push(move)            \n        \n        if self.is_game_over():\n            score_after_move = eval_score\n        else:\n            # Quiescence search - don't end on a move that isn't \"quiet\",\n            # i.e. if a capture took place. This makes sure that potential\n            # re-captures are taken into account.\n            if iteration == max_iterations - 1 and is_capture:\n                new_iteration = iteration\n            else:\n                new_iteration = iteration + 1\n\n            score_after_move, _, used_endgame = self.get_best_move_alphabeta(\n                    cache, use_heuristic, alpha, beta, not ai_turn, not color,\n                    endgame_tablebase, new_iteration, max_iterations,\n                    using_progressive_deepening, eval_score)\n            used_endgame_anywhere = used_endgame_anywhere or used_endgame\n                \n        self.pop()          \n                                                   \n        if (ai_turn and score_after_move > best_score) \\\n                or (not ai_turn and score_after_move < best_score):\n            best_score = score_after_move\n            best_move = move\n            best_move_used_endgame = used_endgame_anywhere\n\n        if ai_turn:\n            if best_score >= beta:\n                return best_score, best_move, best_move_used_endgame\n            if best_score > alpha:\n                alpha = best_score\n        else:\n            if best_score <= alpha:\n                return best_score, best_move, best_move_used_endgame\n            if best_score < beta:\n                beta = best_score\n                \n    return best_score, best_move, best_move_used_endgame  \n\nchess.Board.evaluate_moves_alphabeta = evaluate_moves_alphabeta\ndel evaluate_moves_alphabeta  ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Memoization For Alpha-Beta Pruning\nBecause the same state on the same iteration may yield a different score if the values for α and β are different, it is required to include α and β in the cache. While it would be correct to simply store these values in the cache and only use the cached values if α and β are equal to their cached counterparts, this is unoptimal. Because chess evaluation scores can vary so greatly, it is unlikely for α and β to be exactly equal to their cached values, meaning that the cache can rarely be used. As described earlier, α and β are bounds for evaluation scores. This property can be used to improve the percentage of cache hits while still producing a correct score, see Stroetmann, K. (2022).\n\nCached values are tuples $(flag, v)$. $v$ is the cached evaluation score. $flag$ can be $≤$, $=$, or $≥$. Given a dictionary $cache$, this tuple represents the following:\n* $\\texttt{cache[s]} = ('=', v) \\rightarrow \\texttt{value(s)} = v$.\n* $\\texttt{cache[s]} = ('≤', v) \\rightarrow \\texttt{value(s)} \\leq v$.\n* $\\texttt{cache[s]} = ('≥', v) \\rightarrow \\texttt{value(s)} \\geq v$.\n\nWe define a function $evaluate$ that takes a state $s$, a function $f$ (either $alphaBetaMax$ or $alphaBetaMin$), an alpha value $α$ and a beta value $β$. This function returns the score of board state $s$ given optimal play. Given this function, the following is true:\n\n1. $\\texttt{cache[s]} = ('=', v) \\rightarrow \\texttt{evaluate}(\\texttt{s}, f, \\alpha, \\beta) = v.$\n1. $\\texttt{cache[s]} = ('≤', v) \\wedge v \\leq \\alpha \\rightarrow \\texttt{evaluate}(\\texttt{s}, f, \\alpha, \\beta) = v.$\n1. $\\texttt{cache[s]} = ('≤', v) \\wedge \\alpha < v < \\beta \\rightarrow  \\texttt{evaluate}(\\texttt{s}, f, \\alpha, \\beta) = f(\\texttt{s}, \\alpha, v).$\n1. $\\texttt{cache[s]} = ('≤', v) \\wedge \\beta \\leq v \\rightarrow  \\texttt{evaluate}(\\texttt{s}, f, \\alpha, \\beta) = f(\\texttt{s}, \\alpha, \\beta).$\n1. $\\texttt{cache[s]} = ('≥', v) \\wedge \\beta \\leq v \\rightarrow  \\texttt{evaluate}(\\texttt{s}, f, \\alpha, \\beta) = v.$\n1. $\\texttt{cache[s]} = ('≥', v) \\wedge \\alpha < v < \\beta \\rightarrow  \\texttt{evaluate}(\\texttt{s}, f, \\alpha, \\beta) = f(\\texttt{s}, v, \\beta).$\n1. $\\texttt{cache[s]} = ('≥', v) \\wedge v \\leq \\alpha \\rightarrow \\texttt{evaluate}(\\texttt{s}, f, \\alpha, \\beta) = f(\\texttt{s}, \\alpha, \\beta).$\n\n---",
   "metadata": {
    "cell_id": "b58f32337cbd4b9aab3311e1e70456d0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 621.6666870117188
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Comparisons Class\nAn enum that contains values for the following comparisons:\n- Equal to\n- Less than or equal to\n- Greater than or equal to\n\nThese values are used for alpha-beta pruning memoization and replace the single-character strings mentioned in the theory above, as integer constants are more efficient.",
   "metadata": {
    "cell_id": "4e159d1e49a74d4aa517d1f1ee3053e0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 246.23333740234375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b2eafaee-876b-4f32-a562-403ea177a709",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 42
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1646752647644,
    "source_hash": "f848c8ab",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 134.8333282470703
   },
   "source": "class Comparisons(Enum):\n    EQUAL = 0\n    LTE = 1 # Less than or equal to\n    GTE = 2 # Greater than or equal to",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.get_alphabeta_cache\nRetrieves a cached value in an alpha-beta pruning search, based on the 7 equations listed in the theory above.\n\n###### __<u>Arguments</u>__\n``cache (dict):``  \nThe cache dictionary to read.  \n\n``iteration (int):``  \nThe depth of the current node that's being evaluated.  \n\n``max_iterations (int):``  \nThe maximum search depth used in the alpha-beta pruning search.  \n\n``state (str):``  \nA representation of the current chess board state.  \n\n``alpha (int):``  \nThe \"alpha\" value in the current iteration.  \n\n``beta (int):``  \nThe \"beta\" value in the current iteration.\n\n###### __<u>Returns _(int, int, (int, chess.Move, bool))_</u>__\n- The new \"alpha\" value.\n- The new \"beta\" value.\n- The move score, move and whether or not the endgame tablebase was used; or None if no value could be read from the cache.",
   "metadata": {
    "cell_id": "0ac11086985745db8f36a419c11fa943",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 590.0333251953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a6e40fb8-e9fc-48a1-8828-a6bb03a80018",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 48
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1646752647657,
    "source_hash": "610c5561",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 728.8333129882812
   },
   "source": "def get_alphabeta_cache(\n    self,\n    cache: dict,\n    iteration: int,\n    max_iterations: int,\n    state: str,\n    alpha: int,\n    beta: int\n) -> (int, int, (int, chess.Move, bool)):\n\n    # Note: if a result is returned (i.e. not None) then alpha and beta\n    # don't matter anymore\n\n    key = (iteration, max_iterations, state)\n    if key not in cache:\n        return alpha, beta, None\n\n    flag, result = cache[key]\n    score, _, _ = result\n    if flag == Comparisons.EQUAL:\n        return alpha, beta, result\n    if flag == Comparisons.LTE:\n        if score <= alpha:\n            return alpha, beta, result\n        if score < beta: # alpha < score < beta\n            return alpha, score, None\n    if flag == Comparisons.GTE:\n        if beta <= score:\n            return alpha, beta, result\n        if alpha < score: # alpha < score < beta\n            return score, beta, None\n\n    # Invalid flag or no optimization possible\n    return alpha, beta, None\n\nchess.Board.get_alphabeta_cache = get_alphabeta_cache\ndel get_alphabeta_cache",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.store_alphabeta_cache\nCaches a board score in an alpha-beta pruning search, based on the representation of the cached tuples as described in the theory above.\n\n###### __<u>Arguments</u>__\n``cache (dict):``  \nThe cache dictionary to write to.  \n\n``iteration (int):``  \nThe depth of the current node that's being evaluated.  \n\n``max_iterations (int):``  \nThe maximum search depth used in the alpha-beta pruning search.  \n\n``state (str):``  \nA representation of the current chess board state.  \n\n``alpha (int):``  \nThe \"alpha\" value in the current iteration.  \n\n``beta (int):``  \nThe \"beta\" value in the current iteration.  \n\n``result (tuple(int, chess.Move, bool)):``  \nThe move score, move and whether or not the endgame tablebase was used.",
   "metadata": {
    "cell_id": "de16a7b63ffa4b968a9c7ad3420d6304",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 544.63330078125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0f087a79-7cbb-4bef-95de-5562749be1a6",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 54
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1646752647672,
    "source_hash": "a374430c",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 476.83331298828125
   },
   "source": "def store_alphabeta_cache(\n    self,\n    cache: dict,\n    iteration: int,\n    max_iterations: int,\n    state: str,\n    alpha: int,\n    beta: int,\n    result: tuple\n) -> None:\n\n    key = (iteration, max_iterations, state)\n    score, _, _ = result\n\n    if score <= alpha:\n        cache[key] = (Comparisons.LTE, result)\n    elif score < beta: # alpha < score < beta\n        cache[key] = (Comparisons.EQUAL, result)\n    else: # beta <= score\n        cache[key] = (Comparisons.GTE, result)\n\nchess.Board.store_alphabeta_cache = store_alphabeta_cache\ndel store_alphabeta_cache",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.alphabeta_value_cache\nReturns a cached move score if available, or a score of 0 if not. Can be used to sort moves by score.\n\n###### __<u>Arguments</u>__\n``cache (dict):``  \nThe cache dictionary to read.  \n\n``move (chess.Move):``  \nThe move whose potentially cached score should be found.  \n\n``iteration (int):``  \nThe current depth in the alpha-beta pruning search.\n\n``max_iterations (int):``  \nThe maximum search depth used in the alpha-beta pruning search.\n\n###### __<u>Returns _(int)_</u>__\nThe cached move score, or 0 if unavailable.",
   "metadata": {
    "cell_id": "dec4c94057eb400e83ff27f44ee39422",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 404.6333312988281
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c7bc5d80-447d-4f93-bf20-fbb0f244aee5",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 60
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1646752647682,
    "source_hash": "59151123",
    "tags": [],
    "owner_user_id": "bf601137-8aff-4afc-8e92-5767ca086e79",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 422.83331298828125
   },
   "source": "def alphabeta_value_cache(\n    self,\n    cache: dict,\n    move: chess.Move,\n    iteration: int,\n    max_iterations: int\n) -> int:\n    if max_iterations < 1:\n        return 0\n        \n    self.push(move)\n    state = self.get_state_repr()\n    self.pop()\n    \n    _, result = cache.get((0, max_iterations, state), ('=', (0, None, False)))\n    score, _, _ = result\n    return score\n\nchess.Board.alphabeta_value_cache = alphabeta_value_cache\ndel alphabeta_value_cache",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### make_move_alphabeta\nFinds the best possible move according to the alpha-beta pruning algorithm and pushes it onto the move stack.\n\n###### <b><u>Arguments</u></b>\n``board (chess.Board):``  \nThe board to push the move to.  \n\n``color (chess.Color):``  \nThe color of the player that makes the move.  \n\n``search_depth (int):``  \nThe iteration depth of the alpha-beta search.  \n\n``endgame_tablebase (Union[chess.gaviota.NativeTablebase, chess.gaviota.PythonTablebase]):``  \nThe endgame tablebase attached to the game, which serves as a shortcut for ideal moves in the endgame.  \n\n``use_heuristic (bool):``  \nWhether or not the heuristic for evaluating the chess board should be used. Chess problems don't need this heuristic.\n\n###### __<u>Returns _(int, chess.Move, bool)_</u>__\n- The evaluated score of the best possible move.\n- The best possible move that was found.\n- Whether or not the endgame library was used to find the move.\n\n###### __<u>Side effects</u>__\n- The best possible move is pushed to the move stack of the board.",
   "metadata": {
    "cell_id": "00011-23576796-2219-48b5-9b85-338d5304a3fa",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 72
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 593.0333251953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-f54aaa33-8247-43b3-9098-2890db438a8d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 78
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1646752647719,
    "source_hash": "4ca7d759",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 530.8333129882812
   },
   "source": "def make_move_alphabeta(\n    board: chess.Board,\n    color: chess.Color,\n    search_depth: int,\n    endgame_tablebase: Union[chess.gaviota.NativeTablebase,\n            chess.gaviota.PythonTablebase],\n    use_heuristic: bool,\n) -> (int, chess.Move, bool):\n\n    score, move, used_endgame = board.get_best_move_alphabeta(\n        cache={}, \n        use_heuristic=use_heuristic,\n        alpha=-Globals.MAX_EVALUATION_SCORE, \n        beta=Globals.MAX_EVALUATION_SCORE, \n        ai_turn=True, \n        color=color,\n        endgame_tablebase=endgame_tablebase,\n        iteration=0, \n        max_iterations=search_depth,\n        using_progressive_deepening=False,\n        last_eval_score=0 # The starting board score doesn't matter,\n                          # it's evaluated by score difference\n    )\n    \n    board.push(move)\n    return score, move, used_endgame",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Progressive deepening\n\n__Progressive deepening__, sometimes known as iterative deepening, is the idea of gradually inreasing the maximum search depth up to a certain number. In the context of games, this method can be used if information from a previous search is useful for a current search, even if that previous search used a lower search depth.\n\nStroetmann, K. (2022) explains that this is the case in alpha-beta pruning. In alpha-beta pruning, it is advantageous to evaluate the best moves first. These moves change the values of α and β the most significantly. Therefore, more branches of the search can be pruned, decreasing the amount of nodes to evaluate and speeding up the search. If the move scores of previous searches in a progressive deepening implementation are cached, these values can be used to estimate the effectiveness of each move, allowing the current search to sort the move list using these scores. This creates a best-first ordered list of moves, allowing the aforementioned optimization of alpha-beta pruning.\n\nBecause the amount of nodes that a search needs to evaluate grows exponentially with its search depth, searching the tree with a lower search depth takes comparatively little time. Generally, the time saved from sorting moves outweighs the time spent on performing these extra searches.\n\n---",
   "metadata": {
    "cell_id": "829f866b102e4bdab8cbbe43927ecdd6",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 390.4666748046875
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### make_move_alphabeta_progressive_deepening\nFinds the best possible move according to the alpha-beta pruning algorithm and pushes it onto the move stack. Utilizes progressive deepening by iteratively increasing the maximum search depth.\n\n###### __<u>Arguments</u>__\n``board (chess.Board):``  \nThe board to push the move to.  \n\n``color (chess.Color):``  \nThe color of the player that makes the move.  \n\n``search_depth (int):``  \nThe iteration depth of the alpha-beta search.  \n\n``endgame_tablebase (Union[chess.gaviota.NativeTablebase, chess.gaviota.PythonTablebase]):``  \nThe endgame tablebase attached to the game, which serves as a shortcut for ideal moves in the endgame.  \n\n``use_heuristic (bool):``  \nWhether or not the heuristic for evaluating the chess board should be used. Chess problems don't need this heuristic.\n\n###### __<u>Returns _(int, chess.Move, bool)_</u>__\n- The evaluated score of the best possible move.\n- The best possible move that was found.\n- Whether or not the endgame library was used to find the move.\n\n###### __<u>Side effects</u>__\n- The best possible move is pushed to the move stack of the board.",
   "metadata": {
    "cell_id": "18b9e12113214f8080ad21668e8cc5e7",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 615.433349609375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3f145f6b-a43b-4ed2-b0f6-485e5aed44a4",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 84
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1646752647719,
    "source_hash": "699f9d1",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 710.8333129882812
   },
   "source": "def make_move_alphabeta_progressive_deepening(\n    board: chess.Board,\n    color: chess.Color,\n    search_depth: int,\n    endgame_tablebase: Union[chess.gaviota.NativeTablebase,\n            chess.gaviota.PythonTablebase],\n    use_heuristic: bool,\n) -> (int, chess.Move, bool):\n\n    cache = {}\n    score, move, used_endgame = 0, None, False\n\n    for limit in range(1, search_depth + 1):\n        score, move, used_endgame = board.get_best_move_alphabeta(\n            cache=cache,\n            use_heuristic=use_heuristic,\n            alpha=-Globals.MAX_EVALUATION_SCORE,\n            beta=Globals.MAX_EVALUATION_SCORE,\n            ai_turn=True,\n            color=color,\n            endgame_tablebase=endgame_tablebase,\n            iteration=0,\n            max_iterations=limit,\n            using_progressive_deepening=True,\n            last_eval_score=0 # The starting board score doesn't matter,\n                              # it's evaluated by score difference\n        )\n\n        # If a checkmate in favor of the AI has been found, there's no\n        # reason to search any further\n        if score >= Globals.EVALUATION_SCORE_CHECKMATE - limit:\n            board.push(move)\n            return score, move, used_endgame\n\n    board.push(move)\n    return score, move, used_endgame",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### chess.Board.get_sorted_moves\nReturns a list of legal moves, sorted by move score, descending. This lets alpha-beta pruning search through the best move trees first.\n\n###### __<u>Arguments</u>__\n``cache (dict):``  \nThe cache dictionary to read.  \n\n``ai_turn (bool):``  \nWhether or not it's the turn of the AI that started the search.  \n\n``iteration (int):``  \nThe current depth in the alpha-beta pruning search.\n\n``max_iterations (int):``  \nThe maximum search depth used in the alpha-beta pruning search.\n\n###### __<u>Returns _(list)_</u>__\nThe sorted list of legal moves.",
   "metadata": {
    "cell_id": "d94d0b48087b4acd9ec45bfcd35ed572",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 427.0333251953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3768c34c-fd6b-4f6e-a31f-c99755b5bd9d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 66
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1646752647718,
    "source_hash": "d509e800",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 350.83331298828125
   },
   "source": "def get_sorted_moves(\n    self,\n    cache: dict,\n    ai_turn: bool,\n    iteration: int,\n    max_iterations: int\n) -> list:\n    return sorted(\n        self.legal_moves,\n        reverse=ai_turn,\n        key=lambda move: self.alphabeta_value_cache(cache, move,\n                iteration, max_iterations - 2)\n    )\n\nchess.Board.get_sorted_moves = get_sorted_moves\ndel get_sorted_moves",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00032-c38288a6-a7cb-486c-8a4a-c0e4ea31b961",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 65.83332824707031
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d6ce9acd-52c5-4422-904d-8424da19408b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a3aa8d62-1517-4618-8cd7-29b3aa7b5c6c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 }
}