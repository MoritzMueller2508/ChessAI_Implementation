{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-2177f857-5779-403f-91bc-a18673125b8d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": null
    },
    "deepnote_cell_height": 116.75,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     2
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16003,
    "execution_start": 1652801061749,
    "source_hash": "bd190a5a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100%; !important } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100%; !important } </style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-c29ba798-cd7a-4e45-8422-6ee928df7b27",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 6
    },
    "deepnote_cell_height": 81.83332824707031,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Negamax Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1774b82a0e0541a6a14a3f414f8c3212",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 419.433349609375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Overview\n",
    "According to [Knuth, D. E. and Moore, R. W. (1975)](Bibliography.ipynb#KM75), the **negamax search algorithm** is a simple algorithm for choosing an optimal move in a two-person zero-sum game with perfect information.\n",
    "\n",
    "As the name implies, the negamax search algorithm utilizes a search tree. In this tree, each node represents a state of the game and the player who can make a move in that state. The edges of the tree represents moves that transition one state into another. Therefore, if a node representing state $\\texttt{s} \\in \\texttt{States}$ and player $\\texttt{p} \\in \\texttt{Players}$ has $\\texttt{n} \\in \\mathbb{N} \\cup \\{0\\}$ children, those child nodes represent the states $\\texttt{ns}_1, ..., \\texttt{ns}_\\texttt{n}$ that can be reached when $\\texttt{p}$ makes a legal move in $\\texttt{s}$.\n",
    "\n",
    "Negamax search assumes that both players in the game play optimally. In other words, each player is assumed to choose the best available legal move in any given game state. Because the $\\texttt{heuristic}$ function returns negated scores for the opponent player (see [Introduction.ipynb](Introduction.ipynb#Heuristics)), the original player chooses the move with the highest score whereas the opponent player chooses the move with the lowest score. This is slightly different from the negamax search algorithm described by [Knuth, D. E. and Moore, R. W. (1975)](Bibliography.ipynb#KM75), where both players try to maximize the negated scores from their opponent. These two methods are functionally equivalent, but the former method allows scores to always represent the effectiveness of a move from the perspective of the player that started the search. This makes the scores more intuitive, therefore our implementation uses this method, despite carrying the drawback of making the implementation slightly more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5f0379011cab496796444c70f5d89703",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 422.23333740234375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Switching Turns\n",
    "For negamax search, we require functions to switch turns. This involves both switching the current player and switching whether or not that player is the opponent. We call that first function $\\texttt{switchPlayer}$, which has the following signature:\n",
    "\n",
    "$$\\texttt{switchPlayer: Players} \\rightarrow \\texttt{Players}$$\n",
    "\n",
    "and is defined as follows:\n",
    "\n",
    "$$\\texttt{switchPlayer}(\\texttt{Players}[1]) := \\texttt{Players}[2]$$  \n",
    "$$\\texttt{switchPlayer}(\\texttt{Players}[2]) := \\texttt{Players}[1]$$\n",
    "\n",
    "We call the second function $\\texttt{switchOpponent}$, which has the following signature:\n",
    "\n",
    "$$\\texttt{switchOpponent:} \\{-1, 1\\} \\rightarrow \\{-1, 1\\}$$\n",
    "\n",
    "and is defined as follows:\n",
    "\n",
    "$$\\texttt{switchOpponent}(1) := -1$$  \n",
    "$$\\texttt{switchOpponent}(-1) := 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab0838d53e09487b8f469860c94d9f10",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 294.0333251953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Paths\n",
    "We define a *path* as a list $[(\\texttt{s}_1, \\texttt{m}_1, \\texttt{p}_1, \\texttt{o}_1), ..., (\\texttt{s}_\\texttt{n}, \\texttt{m}_\\texttt{n}, \\texttt{p}_\\texttt{n}, \\texttt{o}_\\texttt{n})]$ where each element is a tuple $\\texttt{e} \\in \\texttt{States} \\times \\texttt{Moves} \\times \\texttt{Players} \\times \\{-1, 1\\}$ representing information at a search node. Here, for all $\\texttt{i} \\in \\{1, ..., \\texttt{n} - 1\\}$, the following statements hold:\n",
    "- $\\texttt{s}_{\\texttt{i}+1} = \\texttt{nextState}(\\texttt{s}_\\texttt{i}, \\texttt{m}_\\texttt{i}, \\texttt{p}_\\texttt{i})$\n",
    "- $\\texttt{m}_\\texttt{i} \\in \\texttt{legalMoves}(\\texttt{s}_\\texttt{i}, \\texttt{p}_\\texttt{i})$\n",
    "- $\\texttt{p}_{\\texttt{i}+1} = \\texttt{switchPlayer}(\\texttt{p}_\\texttt{i})$\n",
    "- $\\texttt{o}_{\\texttt{i}+1} = \\texttt{switchOpponent}(\\texttt{o}_\\texttt{i})$\n",
    "\n",
    "A path represents a path down the search tree. We define $\\texttt{Paths}$ as the set of all valid paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dc470d05fe994194b2fd150bc54b9e16",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 240.23333740234375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Finding the Best Value: Idea\n",
    "We declare a function $\\texttt{bestValue}$ with the following signature:\n",
    "\n",
    "$$\\texttt{bestValue: States} \\times \\texttt{Players} \\times \\{-1, 1\\} \\times \\mathbb{N} \\cup \\{0\\} \\times \\texttt{Paths} \\rightarrow \\mathbb{Z}$$\n",
    "\n",
    "$\\texttt{bestValue}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path})$ denotes the greatest value achievable from state $\\texttt{s}$ given optimal play from both players, from the perspective of the player $\\texttt{p}$ who can currently make a move. $\\texttt{o} \\in \\{-1, 1\\}$ is $\\texttt{-1}$ if $\\texttt{p}$ is the opponent player and $\\texttt{1}$ otherwise. $\\texttt{d} \\in \\mathbb{N} \\cup \\{0\\}$ represents the current search depth, i.e. the number of half-moves to look into the future from the current position. $\\texttt{path} \\in \\texttt{Paths}$ represents the path leading up to the current node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d2124db0bb4d40efbc5260fe13ef4c57",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 414.7833251953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Finding the Next Values\n",
    "As a helper function, we define the function $\\texttt{nextValues}$ with the following signature:\n",
    "\n",
    "$$\\texttt{nextValues: States}\\times \\texttt{Players} \\times \\{-1, 1\\} \\times \\mathbb{N} \\times \\texttt{Paths} \\rightarrow 2^\\mathbb{Z}$$\n",
    "\n",
    "$\\texttt{nextValues}$ returns the set of values from calling $\\texttt{bestValue}$ on all states in the next layer in the search tree, i.e. on all states that can be reached when player $\\texttt{p}$ makes a legal move in state $\\texttt{s}$. This is the set that the player $\\texttt{p}$ picks the highest or lowest value from, depending on whether or not they're the opponent. We define $\\texttt{nextValues}$ as follows:\n",
    "\n",
    "$\n",
    "\\begin{array}{l}\n",
    "\\texttt{nextValues}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}) := \\{ \\\\\n",
    "    \\hspace{10mm} \\texttt{bestValue}( \\\\\n",
    "    \\hspace{20mm} \\texttt{nextState}(\\texttt{s}, \\texttt{m}, \\texttt{p}), \\\\\n",
    "    \\hspace{20mm} \\texttt{switchPlayer}(\\texttt{p}), \\\\\n",
    "    \\hspace{20mm} \\texttt{switchOpponent}(\\texttt{o}), \\\\\n",
    "    \\hspace{20mm} \\texttt{d} - 1, \\\\\n",
    "    \\hspace{20mm} \\texttt{path} + [(\\texttt{s}, \\texttt{m}, \\texttt{p}, \\texttt{o})] \\\\\n",
    "    \\hspace{10mm} ) \\mid \\texttt{m} \\in \\texttt{legalMoves}(\\texttt{s}, \\texttt{p}) \\\\\n",
    "\\}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "148531639e3a4d2fb1d94ede31491f30",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 291.816650390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Path Score\n",
    "If the search depth $\\texttt{d}$ reaches $0$ or the game has finished, we return the combined value from the $\\texttt{heuristic}$ function for the move sequence leading up to the current of the search tree. In other words, this is the combined score for all moves in the path leading up to the current node. For this, we define the function $\\texttt{pathScore}$ with the following signature:\n",
    "\n",
    "$$\\texttt{pathScore: Paths} \\rightarrow \\mathbb{Z}$$\n",
    "\n",
    "We define $\\texttt{pathScore}$ as follows:\n",
    "\n",
    "$$ \\texttt{pathScore}(\\texttt{path}) := \\sum_{i=1}^{\\texttt{length}(\\texttt{path})} \\texttt{heuristic}(\\texttt{s}_\\texttt{i}, \\texttt{m}_\\texttt{i}, \\texttt{p}_\\texttt{i}, \\texttt{o}_\\texttt{i}) $$\n",
    "\n",
    "where $\\texttt{path}[\\texttt{i}] = (\\texttt{s}_\\texttt{i}, \\texttt{m}_\\texttt{i}, \\texttt{p}_\\texttt{i}, \\texttt{o}_\\texttt{i})$ $\\forall \\texttt{i} \\in \\{1, ..., \\texttt{length}(\\texttt{path})\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c8dcdc1ba4e1464f95a35e79b9de9d81",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 254.61666870117188,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Finding the Best Value: Definition\n",
    "If the search depth is greater than $0$ and the game has not finished, $\\texttt{bestValue}$ evaluates the moves at the next depth, maximizing or minimizing the score depending on whether or not $\\texttt{p}$ is the opponent player, as depicted by the value $\\texttt{o}$. Otherwise, it returns the path score as defined above to approximate how effective this sequence of moves is. With that, we can define $\\texttt{bestValue}$ as follows:\n",
    "\n",
    "$\n",
    "\\texttt{bestValue}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path}) = \n",
    "\\begin{cases}\n",
    "\\texttt{pathScore}(\\texttt{path}) & \\texttt{if finished}(\\texttt{s}) \\vee \\texttt{d} = 0 \\\\\n",
    "\\max(\\{\\texttt{nextValues}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path})\\}) & \\texttt{if } \\neg \\texttt{finished}(\\texttt{s}) \\wedge \\texttt{d} > 0 \\wedge \\texttt{o} = 1 \\\\\n",
    "\\min(\\{\\texttt{nextValues}(\\texttt{s}, \\texttt{p}, \\texttt{o}, \\texttt{d}, \\texttt{path})\\}) & \\texttt{if } \\neg \\texttt{finished}(\\texttt{s}) \\wedge \\texttt{d} > 0 \\wedge \\texttt{o} = -1 \\\\\n",
    "\\end{cases} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c39e60785f9417dbb0c0d740d6e3af5",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 587.8333129882812,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Our Implementation\n",
    "In our implementation, $\\texttt{bestValue}$ is called `NegamaxSearch.get_best_move`. On top of the score of the optimal move, it also returns that optimal move as well as whether or not the endgame tablebase was used to find the move. It has the following arguments:\n",
    "- `ai_turn`: Whether or not it's the turn of the player (AI) that started the search. This has the same functionality as the parameter $\\texttt{o}$, where `ai_turn = True` is equivalent to $\\texttt{o} = 1$ and `ai_turn = False` is equivalent to $\\texttt{o} = -1$.\n",
    "- `depth`: The remaining search depth. Identical to the parameter $\\texttt{d}$.\n",
    "- `last_eval_score`: The move score tied to the parent node. Our heuristic function is iterative, see [Evaluation.ipynb](Evaluation.ipynb#EvaluateMove). Therefore, the search needs to keep track of the score as it moves down the tree, rather than calculating the score at a leaf node. This uses the same idea as the function $\\texttt{pathScore}$.\n",
    "\n",
    "The parameters $\\texttt{s}$ and $\\texttt{p}$ are derived from `self.board` and `self.board.turn`, as this is a function of the `NegamaxSearch` class, which inherits from the `ChessAlgorithm` class and therefore has access to a `chess.Board` object containing the chess board state. The value $\\texttt{path}$ is represented by the variable `best_move_path`. This only keeps track of the moves in the path, as the other elements are represented by the arguments and variables listed above and are updated throughout the search.\n",
    "\n",
    "Optionally, this function uses **memoization**, sometimes called **transposition tables**, which is a type of caching. The idea is that, when a certain board state at a certain search depth is evaluated using the heuristic function, this result can be cached. If, later in the search, this board state is reached again at the same search depth, the result can simply be read from the cache, rather than having to evaluate it again.\n",
    "\n",
    "Furthermore, this function implements **singular value extension**. If there's only one legal move to be made, the computation time of the current node is neglegible. Therefore, if this is the case, this node does not count towards the search depth, and the algorithm effectively searches one layer deeper into this branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-dcc47b7a-49e6-4ca6-b283-cd23526a06c4",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 12
    },
    "deepnote_cell_height": 61.83332824707031,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-200438c0-dc12-44fb-acf6-57aa4144be49",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 18
    },
    "deepnote_cell_height": 206.8333282470703,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1199,
    "execution_start": 1652801061815,
    "output_cleared": true,
    "source_hash": "746cf9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chess\n",
    "import random\n",
    "\n",
    "import import_ipynb\n",
    "import Util\n",
    "import Evaluation\n",
    "from Globals import *\n",
    "from ChessAlgorithm import ChessAlgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "500475c7a7a347418c3233c435ea7473",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 100.23333740234375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### NegamaxSearch Class\n",
    "A class that implements the negamax search algorithm as defined above, without memoization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b34c6f138700485880ad6ce46c9a4811",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 98.83332824707031,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16401,
    "execution_start": 1652801062955,
    "source_hash": "b383be4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NegamaxSearch(ChessAlgorithm):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-7a7c748f-d95c-4ec5-8bbb-309075e4fcb4",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 24
    },
    "deepnote_cell_height": 565.0333251953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### NegamaxSearch.get_best_move\n",
    "Finds the best move to make based on the aforementioned negamax search algorithm, as well as the associated move score and whether or not the endgame tablebase was used.\n",
    "\n",
    "__This function is implemented recursively.__\n",
    "\n",
    "###### __<u>Arguments</u>__\n",
    "\n",
    "``ai_turn (bool):``  \n",
    "Whether or not the current turn is of the AI that started the search. If this is the case, the score should be maximized. Otherwise, the score should be minimized.   \n",
    "\n",
    "``depth (int):``  \n",
    "The remaining depth of the search, i.e. how many half-moves the search should still look into the future.  \n",
    "\n",
    "``last_eval_score (int):``  \n",
    "The score provided by the previous evaluation in the search. \n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The recommended best move to make.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place.\n",
    "\n",
    "###### __<u>Side effects</u>__\n",
    "If a cache dictionary for memoization is used, new values may be added to this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-89802ca9-b1c9-46e5-ae87-9b8e860c2e24",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 30
    },
    "deepnote_cell_height": 1151.949951171875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5995,
    "execution_start": 1653030913255,
    "output_cleared": false,
    "source_hash": "83ddf673",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_move(\n",
    "    self,\n",
    "    ai_turn: bool,\n",
    "    depth: int,\n",
    "    last_eval_score: int\n",
    ") -> (int, chess.Move, bool, list):\n",
    "\n",
    "    if self.cache is not None:\n",
    "        state = self.board.get_state_repr()\n",
    "        if (depth, state) in self.cache:\n",
    "            return self.cache[(depth, state)]\n",
    "\n",
    "    color = self.board.turn\n",
    "    original_color = color if ai_turn else not color\n",
    "    result_score = self.board.get_search_result_if_finished(original_color,\n",
    "            depth, last_eval_score)\n",
    "    if result_score is not None:\n",
    "        return result_score, None, False, []\n",
    "\n",
    "    best_score = Globals.MAX_EVALUATION_SCORE * (-1 if ai_turn else 1)\n",
    "    best_move = None\n",
    "    best_move_used_endgame = False\n",
    "    best_move_path = []\n",
    "    use_singular_value_extension = (len(list(self.board.legal_moves)) == 1)\n",
    "    \n",
    "    for move in self.board.legal_moves:\n",
    "        eval_score, used_endgame_anywhere = self.board.evaluate_move(\n",
    "                self.use_heuristic, not ai_turn, last_eval_score,\n",
    "                depth, move, self.endgame_tablebase)\n",
    "        self.board.push(move)\n",
    "        \n",
    "        if self.board.is_game_over():\n",
    "            score_after_move = eval_score\n",
    "            new_move_path = []\n",
    "        else:\n",
    "            new_depth = depth if use_singular_value_extension else depth - 1\n",
    "            score_after_move, _, used_endgame, new_move_path = self.get_best_move(\n",
    "                    not ai_turn, new_depth, eval_score)\n",
    "            used_endgame_anywhere = used_endgame_anywhere or used_endgame\n",
    "\n",
    "        self.board.pop()\n",
    "\n",
    "        if (ai_turn and score_after_move > best_score) \\\n",
    "                or (not ai_turn and score_after_move < best_score):\n",
    "            best_score = score_after_move\n",
    "            best_move = move\n",
    "            best_move_used_endgame = used_endgame_anywhere\n",
    "            best_move_path = [move] + new_move_path\n",
    "\n",
    "    if self.cache is not None:\n",
    "        cache_key = (depth, self.board.get_state_repr())\n",
    "        self.cache[cache_key] = best_score, best_move, best_move_used_endgame, best_move_path\n",
    "\n",
    "    return best_score, best_move, best_move_used_endgame, best_move_path\n",
    "\n",
    "NegamaxSearch.get_best_move = get_best_move\n",
    "del get_best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-e2bc0d01-d112-4903-958d-9cbc4466983d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 36
    },
    "deepnote_cell_height": 240.23333740234375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### NegamaxSearch.make_move\n",
    "Finds the best possible move according to the negamax search algorithm and pushes it onto the move stack.\n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The move that was made.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-917b88be-6d67-4d5c-aeac-7051f3c7e5fc",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 42
    },
    "deepnote_cell_height": 359.95001220703125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 606,
    "execution_start": 1653038408715,
    "source_hash": "9a328879",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_move(self) -> (int, chess.Move, bool, list):\n",
    "    score, move, used_endgame, move_path = self.get_best_move(\n",
    "        ai_turn=True, \n",
    "        depth=self.search_depth,\n",
    "        last_eval_score=0 # The starting board score doesn't matter,\n",
    "                          # it's evaluated by score difference\n",
    "    )\n",
    "    self.board.push(move)\n",
    "\n",
    "    return score, move, used_endgame, move_path\n",
    "\n",
    "NegamaxSearch.make_move = make_move\n",
    "del make_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9420754d8fec439eaaeae76149929352",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 100.23333740234375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### NegamaxSearchMemo Class\n",
    "A class that implements the negamax search algorithm as defined above, including memoization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "eb86a46c5a0d415182a2affda14c676d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_height": 98.83332824707031,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1652801063103,
    "source_hash": "1c20c5b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NegamaxSearchMemo(NegamaxSearch):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-f20586a7-8b85-4b84-9634-38b7a3128476",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 48
    },
    "deepnote_cell_height": 458.6333312988281,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### NegamaxSearchMemo.make_move\n",
    "Finds the best possible move according to the negamax search algorithm and pushes it onto the move stack. The algorithm is optimized using memoization.\n",
    "\n",
    "###### __<u>Returns _(int, chess.Move, bool, list<chess.Move>)_</u>__\n",
    "- The evaluated score of the best possible move.\n",
    "- The move that was made.\n",
    "- Whether or not the endgame tablebases were used to find the move.\n",
    "- The predicted move path, i.e. the sequence of moves that the algorithm predicts will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-e316cbb7-808a-476b-85b6-dbe3f6805f0f",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 54
    },
    "deepnote_cell_height": 314.83331298828125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14,
    "execution_start": 1652801063161,
    "owner_user_id": "1e489002-1d0e-4892-8dc4-49d215805343",
    "source_hash": "4382e1c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_move(self) -> (int, chess.Move, bool, list):\n",
    "    self.cache = {}\n",
    "    score, move, used_endgame, move_path = self.get_best_move(\n",
    "        ai_turn=True, \n",
    "        depth=self.search_depth,\n",
    "        last_eval_score=0 # The starting board score doesn't matter,\n",
    "                          # it's evaluated by score difference\n",
    "    )\n",
    "    self.board.push(move)\n",
    "\n",
    "    return score, move, used_endgame, move_path\n",
    "\n",
    "NegamaxSearchMemo.make_move = make_move\n",
    "del make_move"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "4b2bc438-db4f-4a2d-a41c-2db325d56891",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
